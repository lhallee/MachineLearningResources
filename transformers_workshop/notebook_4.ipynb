{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a6f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f212fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597796c2a23d4752aca713fe8d54ccaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3127883a1304894bbdebddcf079f753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fb1014205d452a86e05400d895f1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/valid-00000-of-00001.parquet:   0%|          | 0.00/180k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b47b0e9fe9f4aa2ab50ba4ccc786a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/19.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33e244548f64f95bed0897a7c74e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78194664917a4fa0ba56d39d94fc509d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192e2f2c201c4899b408da7ff25cf411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 10792\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 626\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seqs', 'labels'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('GleghornLab/SS3')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8cdb0",
   "metadata": {},
   "source": [
    "First let's try one hot encoding as our feature inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e41ef6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQTLGQHDFSAGEGLYTHMKALRPDEDRLSPLHSVYVDQWDWERVMGDGERQFSTLKSTVEAIWAGIKATEAAVSEEFGLAPFLPDQIHFVHSQELLSRYPDLDAKGRERAIAKDLGAVFLVGIGGKLSDGHRHDVRAPDYDDWSTPSELGHAGLNGDILVWNPVLEDAFELSSMGIRVDADTLKHQLALTGDEDRLELEWHQALLRGEMPQTIGGGIGQSRLTMLLLQLPHIGQVQAGVWPAAVRESVPSLL\n",
      "DDDCHHHHHHHHHHHHHHHHHHHHHHHCEEECCCCCEEECCCCCCCCCCCCCCCCEECCCCCCCCCEEECCCCCCHHHHHHHHCCCCCCCEEEEEEEEECCCCCCCCCCCCCEEEEEEEEEECCCCCCCHHHHHHHHHHHHHHHHHHHHHHHHHCCCCCCCCCCCEEEEHHHHHHHCCCCCHHHHHHHHHHHHCEEEEECCCCCCCCCCCCCCCCCCCECCCCECCCCCECCEEEEEEEECCCCEEEEEEEEEEECCHHHHHHHHHHHCCCCHHHCHHHHHHHCCCCCCEEEEEEEHHHHHHHHHCCCCHHHCCCCCCCHHHHHHCCCCC\n"
     ]
    }
   ],
   "source": [
    "seq = dataset['train'][0]['seqs']\n",
    "label = dataset['train'][0]['labels']\n",
    "print(seq)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1b6f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0, 'D': 1, 'E': 2, 'H': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {label: i for i, label in enumerate(sorted(set(label)))}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4aee99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf77a8584ec458e9c63322d6d525c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02480a5746a449ed85e5ea7b14a3601a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f72dbdd659d43af9bf26f4ceb6f0ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here's one way to make label vectors\n",
    "dataset = dataset.map(lambda x: {'label_vector': [label_dict[y] for y in x['labels']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8b7a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 24, 'num_labels': 4}\n"
     ]
    }
   ],
   "source": [
    "# This way looks at all examples so nothing is missed\n",
    "# # Collect sequence alphabet\n",
    "all_seqs = []\n",
    "all_labels = []\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    all_seqs += dataset[split][\"seqs\"]\n",
    "    # dataset uses key 'labels'; if 'label' also exists, prefer 'labels'\n",
    "    if \"labels\" in dataset[split].features:\n",
    "        all_labels += dataset[split][\"labels\"]\n",
    "    else:\n",
    "        all_labels += [ex[\"label\"] for ex in dataset[split]]\n",
    "\n",
    "seq_vocab = sorted(list(set(\"\".join(all_seqs))))\n",
    "label_vocab = sorted(list(set(\"\".join(all_labels))))\n",
    "char2idx = {ch: i for i, ch in enumerate(seq_vocab)}\n",
    "label2idx = {ch: i for i, ch in enumerate(label_vocab)}\n",
    "idx2label = {i: ch for ch, i in label2idx.items()}\n",
    "\n",
    "vocab_size = len(seq_vocab)\n",
    "num_labels = len(label_vocab)\n",
    "\n",
    "print({\"vocab_size\": vocab_size, \"num_labels\": num_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f867b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added PAD token to seq vocab.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2698, 157, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloaders with padding and collate\n",
    "PAD = \"<PAD>\"\n",
    "if PAD not in seq_vocab:\n",
    "    seq_vocab = [PAD] + seq_vocab\n",
    "    char2idx = {ch: i for i, ch in enumerate(seq_vocab)}\n",
    "    vocab_size = len(seq_vocab)\n",
    "    print(\"Added PAD token to seq vocab.\")\n",
    "\n",
    "\n",
    "pad_idx = char2idx[PAD]\n",
    "ignore_index = -100\n",
    "\n",
    "\n",
    "def to_indices(s: str, mapper: dict[str, int]) -> list[int]:\n",
    "    return [mapper[c] for c in s]\n",
    "\n",
    "\n",
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.ds = hf_ds\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        seq = item[\"seqs\"]\n",
    "        labs = item[\"labels\"] if \"labels\" in item else item[\"label\"]\n",
    "        return seq, labs\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, labs = zip(*batch)\n",
    "    seq_ids = [torch.tensor(to_indices(s, char2idx), dtype=torch.long) for s in seqs]\n",
    "    lab_ids = [torch.tensor(to_indices(s, label2idx), dtype=torch.long) for s in labs]\n",
    "    max_len = max(x.size(0) for x in seq_ids)\n",
    "\n",
    "    input_ids = torch.full((len(seqs), max_len), fill_value=pad_idx, dtype=torch.long)\n",
    "    labels = torch.full((len(seqs), max_len), fill_value=ignore_index, dtype=torch.long)\n",
    "    attn_mask = torch.zeros((len(seqs), max_len), dtype=torch.long)\n",
    "\n",
    "    for i, (s_ids, l_ids) in enumerate(zip(seq_ids, lab_ids)):\n",
    "        L = s_ids.size(0)\n",
    "        input_ids[i, :L] = s_ids\n",
    "        labels[i, :L] = l_ids\n",
    "        attn_mask[i, :L] = 1  # 1=valid, 0=pad\n",
    "\n",
    "    # Convert to boolean mask where True means MASKED (padding) for the attention module\n",
    "    attn_mask_bool = attn_mask == 0\n",
    "    return input_ids, labels, attn_mask_bool\n",
    "\n",
    "\n",
    "train_ds = HFDatasetWrapper(dataset[\"train\"]) \n",
    "valid_ds = HFDatasetWrapper(dataset[\"valid\"]) \n",
    "test_ds  = HFDatasetWrapper(dataset[\"test\"]) \n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "len(train_loader), len(valid_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f83890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqLabelTransformer(\n",
       "  (input_proj): Linear(in_features=25, out_features=128, bias=True)\n",
       "  (backbone): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=384, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (q_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model wrapper around provided Transformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class SeqLabelTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_labels: int,\n",
    "        hidden_size: int = 128,\n",
    "        n_heads: int = 2,\n",
    "        n_layers: int = 1,\n",
    "        expansion_ratio: float = 2.0,\n",
    "        dropout: float = 0.1,\n",
    "        rotary: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_proj = nn.Linear(vocab_size, hidden_size)\n",
    "        self.backbone = Transformer(hidden_size=hidden_size, n_heads=n_heads, n_layers=n_layers, expansion_ratio=expansion_ratio, dropout=dropout, rotary=rotary)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        # input_ids: (B, L) of token indices\n",
    "        # one-hot -> project\n",
    "        x = F.one_hot(input_ids, num_classes=vocab_size).float()  # (B, L, V)\n",
    "        x = self.input_proj(x)  # (B, L, H)\n",
    "        x = self.backbone(x, attention_mask=attention_mask)  # (B, L, H)\n",
    "        x = self.norm(x)\n",
    "        logits = self.classifier(x)  # (B, L, C)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = SeqLabelTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    num_labels=num_labels,\n",
    "    hidden_size=128,\n",
    "    n_heads=2,\n",
    "    n_layers=1,\n",
    "    expansion_ratio=2.0,\n",
    "    dropout=0.1,\n",
    "    rotary=True\n",
    ").to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8203bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581ca45ba90442e4b4189f4153b3a912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4206, Accuracy: 0.4523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d91be8ed314e5a8108af9caed66a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4452, Accuracy: 0.4697\n",
      "{'epoch': 1, 'train_loss': 1.193, 'train_f1': 0.4206, 'val_loss': 1.1383, 'val_f1': 0.4452}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d2116c9fe84854a723981f988f8660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4261, Accuracy: 0.4564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a34d38cea644fb093a76aa072aef2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4395, Accuracy: 0.4684\n",
      "{'epoch': 2, 'train_loss': 1.1871, 'train_f1': 0.4261, 'val_loss': 1.1365, 'val_f1': 0.4395}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e449a37a1ba546eda7106720739c7507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4261, Accuracy: 0.4570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fea4ec6fd24809b9123377217f95a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4553, Accuracy: 0.4685\n",
      "{'epoch': 3, 'train_loss': 1.1857, 'train_f1': 0.4261, 'val_loss': 1.1352, 'val_f1': 0.4553}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71c44e20b694b8a9287d8dde4077e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4274, Accuracy: 0.4571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7219536782f84ec6a93539c90424a57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4496, Accuracy: 0.4708\n",
      "{'epoch': 4, 'train_loss': 1.1851, 'train_f1': 0.4274, 'val_loss': 1.1361, 'val_f1': 0.4496}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a2a225da69404e9ab94254cb654346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4262, Accuracy: 0.4574\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c8ea1b3994031a51ca83a2d08552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4584, Accuracy: 0.4740\n",
      "{'epoch': 5, 'train_loss': 1.185, 'train_f1': 0.4262, 'val_loss': 1.1365, 'val_f1': 0.4584}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0072c62a3a20410f95c26b35d8488014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4262, Accuracy: 0.4578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3001eb25d4346e59ee4618fb93927b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4411, Accuracy: 0.4732\n",
      "{'epoch': 6, 'train_loss': 1.1851, 'train_f1': 0.4262, 'val_loss': 1.1363, 'val_f1': 0.4411}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4800a62e8a46a2ae2edd644e49cae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4247, Accuracy: 0.4577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b652eecc2e4d3f81f32d2ed65b64ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4495, Accuracy: 0.4706\n",
      "{'epoch': 7, 'train_loss': 1.1839, 'train_f1': 0.4247, 'val_loss': 1.1396, 'val_f1': 0.4495}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123887dd7f224d16b055e21722067e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4240, Accuracy: 0.4580\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d815d0d733a24acea15ef548df4a0252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4411, Accuracy: 0.4732\n",
      "{'epoch': 8, 'train_loss': 1.1842, 'train_f1': 0.424, 'val_loss': 1.1336, 'val_f1': 0.4411}\n",
      "Early stopping at epoch 8 (best val_f1=0.4584)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2977c16c03141e9a8ee584538c3170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3895, Accuracy: 0.4217\n",
      "Test Loss: 1.2608, Test F1: 0.3895, Test Accuracy: 0.4217\n",
      "Test Confusion Matrix:\n",
      "[[2202    0  418 1551]\n",
      " [ 626    0  132  516]\n",
      " [ 715    0  538  984]\n",
      " [ 946    0  529 1940]]\n",
      "Best Validation F1: 0.4584\n"
     ]
    }
   ],
   "source": [
    "# Training & evaluation with early stopping on validation F1\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "\n",
    "def run_epoch(dl, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    for input_ids, labels, attn_mask in tqdm(dl, desc=f\"{'Training' if train else 'Evaluating'}\", leave=False):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = model(input_ids, attention_mask=attn_mask)\n",
    "            # reshape for token-level CE\n",
    "            B, L, C = logits.shape\n",
    "            loss = criterion(logits.view(B*L, C), labels.view(B*L))\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # collect predictions ignoring pad positions\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = labels != ignore_index\n",
    "        all_preds.extend(preds[mask].detach().cpu().tolist())\n",
    "        all_trues.extend(labels[mask].detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dl))\n",
    "    f1 = f1_score(all_trues, all_preds, average='weighted')\n",
    "    acc = accuracy_score(all_trues, all_preds)\n",
    "    conf_mat = confusion_matrix(all_trues, all_preds)\n",
    "    print(f\"F1: {f1:.4f}, Accuracy: {acc:.4f}\")\n",
    "    return avg_loss, f1, acc, conf_mat\n",
    "\n",
    "\n",
    "best_state = None\n",
    "best_val_f1 = -1.0\n",
    "patience = 3\n",
    "stale = 0\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    train_loss, train_f1, train_acc, train_conf_mat = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_f1, val_acc, val_conf_mat = run_epoch(valid_loader, train=False)\n",
    "    scheduler.step()\n",
    "    print({\"epoch\": epoch, \"train_loss\": round(train_loss, 4), \"train_f1\": round(train_f1, 4), \"val_loss\": round(val_loss, 4), \"val_f1\": round(val_f1, 4)})\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_state = deepcopy(model.state_dict())\n",
    "        stale = 0\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (best val_f1={best_val_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "# Load best and evaluate on test\n",
    "assert best_state is not None, \"No best state captured.\"\n",
    "model.load_state_dict(best_state)\n",
    "test_loss, test_f1, test_acc, test_conf_mat = run_epoch(test_loader, train=False)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test F1: {test_f1:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Confusion Matrix:\\n{test_conf_mat}\")\n",
    "print(f\"Best Validation F1: {best_val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343b611",
   "metadata": {},
   "source": [
    "Second, let's try protein language model embeddings as our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a164ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FastEsmModel were not initialized from the model checkpoint at Synthyra/ESM2-8M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_path = 'Synthyra/ESM2-8M'\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).eval()\n",
    "model = model.to(device)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc4a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 11380 new sequences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b5134f8c3a4f38ab7ff9356c5660bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding batches:   0%|          | 0/2845 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11380\n"
     ]
    }
   ],
   "source": [
    "all_seqs = list(set(list(dataset['train']['seqs']) + list(dataset['valid']['seqs']) + list(dataset['test']['seqs'])))\n",
    "all_seqs = sorted(all_seqs, key=len)\n",
    "\n",
    "embedding_dict = model.embed_dataset(\n",
    "    sequences=all_seqs,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4, # adjust for your GPU memory\n",
    "    max_len=512, # adjust for your needs\n",
    "    full_embeddings=True, # if True, no pooling is performed\n",
    "    embed_dtype=torch.float32, # cast to what dtype you want\n",
    "    #pooling_types=['mean', 'cls'], # more than one pooling type will be concatenated together\n",
    "    num_workers=0, # if you have many cpu cores, we find that num_workers = 4 is fast for large datasets\n",
    "    sql=False, # if True, embeddings will be stored in SQLite database\n",
    "    sql_db_path='embeddings.db',\n",
    "    save=True, # if True, embeddings will be saved as a .pth file\n",
    "    save_path='embeddings.pth',\n",
    ")\n",
    "# embedding_dict is a dictionary mapping sequences to their embeddings as tensors for .pth or numpy arrays for sql\n",
    "print(len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75e45672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2698, 157, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding-based DataLoaders (use per-residue embeddings instead of one-hot)\n",
    "assert 'embedding_dict' in globals(), \"Run the embedding cell to create embedding_dict first.\"\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, hf_ds, embedding_lookup: dict):\n",
    "        self.ds = hf_ds\n",
    "        self.lookup = embedding_lookup\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        seq = item['seqs']\n",
    "        labs = item['labels'] if 'labels' in item else item['label']\n",
    "        emb = self.lookup[seq]\n",
    "        if not torch.is_tensor(emb):\n",
    "            emb = torch.tensor(emb)\n",
    "        # Handle potential special tokens (e.g., BOS/EOS) in embeddings\n",
    "        if emb.dim() == 2 and emb.size(0) == len(labs) + 2:\n",
    "            emb = emb[1:-1]\n",
    "        # Align lengths conservatively\n",
    "        K = min(emb.size(0), len(labs))\n",
    "        return emb[:K].float(), labs[:K]\n",
    "\n",
    "\n",
    "def collate_fn_emb(batch):\n",
    "    embs, lab_strs = zip(*batch)\n",
    "    lengths = [e.size(0) for e in embs]\n",
    "    max_len = max(lengths)\n",
    "    embed_dim = embs[0].size(1)\n",
    "\n",
    "    inputs = torch.zeros((len(embs), max_len, embed_dim), dtype=torch.float32)\n",
    "    labels = torch.full((len(embs), max_len), fill_value=ignore_index, dtype=torch.long)\n",
    "    attn_mask = torch.zeros((len(embs), max_len), dtype=torch.long)\n",
    "\n",
    "    for i, (e, labs) in enumerate(zip(embs, lab_strs)):\n",
    "        L = e.size(0)\n",
    "        inputs[i, :L] = e\n",
    "        labels[i, :L] = torch.tensor([label2idx[c] for c in labs], dtype=torch.long)\n",
    "        attn_mask[i, :L] = 1\n",
    "\n",
    "    attn_mask_bool = attn_mask == 0\n",
    "    return inputs, labels, attn_mask_bool\n",
    "\n",
    "train_ds_emb = EmbeddingDataset(dataset['train'], embedding_dict)\n",
    "valid_ds_emb = EmbeddingDataset(dataset['valid'], embedding_dict)\n",
    "test_ds_emb  = EmbeddingDataset(dataset['test'],  embedding_dict)\n",
    "\n",
    "batch_size_emb = 4\n",
    "train_loader_emb = DataLoader(train_ds_emb, batch_size=batch_size_emb, shuffle=True,  collate_fn=collate_fn_emb, num_workers=0)\n",
    "valid_loader_emb = DataLoader(valid_ds_emb, batch_size=batch_size_emb, shuffle=False, collate_fn=collate_fn_emb, num_workers=0)\n",
    "test_loader_emb  = DataLoader(test_ds_emb,  batch_size=batch_size_emb, shuffle=False, collate_fn=collate_fn_emb, num_workers=0)\n",
    "\n",
    "len(train_loader_emb), len(valid_loader_emb), len(test_loader_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0576a99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbSeqLabelTransformer(\n",
       "  (input_proj): Linear(in_features=320, out_features=128, bias=False)\n",
       "  (backbone): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=384, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (q_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=128, out_features=512, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "          (4): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model that consumes embeddings directly\n",
    "class EmbSeqLabelTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_labels: int, hidden_size: int = 128, n_heads: int = 2, n_layers: int = 1, expansion_ratio: float = 2.0, dropout: float = 0.1, rotary: bool = True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(embed_dim, hidden_size, bias=False)\n",
    "        self.backbone = Transformer(hidden_size=hidden_size, n_heads=n_heads, n_layers=n_layers, expansion_ratio=expansion_ratio, dropout=dropout, rotary=rotary)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels, bias=True)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, attention_mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        x = self.input_proj(inputs)\n",
    "        x = self.backbone(x, attention_mask=attention_mask)\n",
    "        x = self.norm(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Infer embed_dim from one sample\n",
    "tmp_e, _ = train_ds_emb[0]\n",
    "embed_dim = tmp_e.size(1)\n",
    "\n",
    "emb_model = EmbSeqLabelTransformer(\n",
    "    embed_dim=embed_dim,\n",
    "    num_labels=num_labels,\n",
    "    hidden_size=128,\n",
    "    n_heads=2,\n",
    "    n_layers=1,\n",
    "    expansion_ratio=2.0,\n",
    "    dropout=0.1,\n",
    "    rotary=True\n",
    ").to(device)\n",
    "emb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77d8e680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99390e30b4654db28d0f50a4e9524427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d49419e7f8452c92a754f35be92aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'train_loss': 0.6829, 'train_f1': 0.7295, 'val_loss': 0.6592, 'val_f1': 0.7364}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74539a403a544783a3fc448c8f499a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a126fc62264146a28742ab3e3a31db0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'train_loss': 0.6431, 'train_f1': 0.7443, 'val_loss': 0.651, 'val_f1': 0.7403}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc02373ccd3a429c9f416392cde1b98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fd91c1067d4675b0b14a2c8e5d4ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'train_loss': 0.6329, 'train_f1': 0.7479, 'val_loss': 0.6438, 'val_f1': 0.7423}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bb83db75634bf29610c1b8d6620ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0943b4651e460b9c5c44152531cf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'train_loss': 0.6262, 'train_f1': 0.7502, 'val_loss': 0.6398, 'val_f1': 0.7439}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3636cf4028f1421d98e1d77d51005d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c9732ada5242e2aa3e9ae1bea56b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'train_loss': 0.6225, 'train_f1': 0.7518, 'val_loss': 0.6407, 'val_f1': 0.7437}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d2636bd8fa4caca3040a7a4cc2f7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b45014c76a84d59bacbea7eef052d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'train_loss': 0.6181, 'train_f1': 0.753, 'val_loss': 0.6371, 'val_f1': 0.745}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81ef0394b894ed78dea120d01b54295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80323aab7366466fba9056959ee00875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'train_loss': 0.6155, 'train_f1': 0.754, 'val_loss': 0.6381, 'val_f1': 0.745}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8846a1f2840a478eb7b4b102336458c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7717a0dd3bf54319abacc1291755653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'train_loss': 0.6144, 'train_f1': 0.7547, 'val_loss': 0.636, 'val_f1': 0.7454}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740c6c5f48d1408faf19baf8bc18c2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea129042ee64b83af72ee37aebba045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'train_loss': 0.6122, 'train_f1': 0.7553, 'val_loss': 0.6378, 'val_f1': 0.7453}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fbd800fe304c4cb2f2db847673d04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ef576cb14f46b290d8807a6fd7f6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'train_loss': 0.6122, 'train_f1': 0.7558, 'val_loss': 0.6361, 'val_f1': 0.7453}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c581e967044fe2b60ba4bd7feecfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (emb):   0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16078781c04474fb30fbeb4e39ba10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 11, 'train_loss': 0.6115, 'train_f1': 0.7557, 'val_loss': 0.6361, 'val_f1': 0.7453}\n",
      "Early stopping (emb) at epoch 11 (best val_f1=0.7454)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e299df91e44c28ab434fce17c3e8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (emb):   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_val_f1_emb': 0.7454, 'test_f1_emb': 0.6645}\n"
     ]
    }
   ],
   "source": [
    "# Training with embeddings + early stopping on validation F1\n",
    "emb_criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "emb_optimizer = torch.optim.AdamW(emb_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "emb_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(emb_optimizer, T_max=10)\n",
    "\n",
    "\n",
    "def run_epoch_emb(dl, train: bool):\n",
    "    if train:\n",
    "        emb_model.train()\n",
    "    else:\n",
    "        emb_model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_trues = [], []\n",
    "    for inputs, labels, attn_mask in tqdm(dl, desc=f\"{'Training' if train else 'Evaluating'} (emb)\", leave=False):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            logits = emb_model(inputs, attention_mask=attn_mask)\n",
    "            B, L, C = logits.shape\n",
    "            loss = emb_criterion(logits.view(B*L, C), labels.view(B*L))\n",
    "        if train:\n",
    "            emb_optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(emb_model.parameters(), 1.0)\n",
    "            emb_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        mask = labels != ignore_index\n",
    "        all_preds.extend(preds[mask].detach().cpu().tolist())\n",
    "        all_trues.extend(labels[mask].detach().cpu().tolist())\n",
    "    avg_loss = total_loss / max(1, len(dl))\n",
    "    f1 = f1_score(all_trues, all_preds, average='weighted')\n",
    "    return avg_loss, f1\n",
    "\n",
    "best_state_emb = None\n",
    "best_val_f1_emb = -1.0\n",
    "patience_emb = 3\n",
    "stale_emb = 0\n",
    "max_epochs_emb = 20\n",
    "\n",
    "for epoch in range(1, max_epochs_emb+1):\n",
    "    train_loss, train_f1 = run_epoch_emb(train_loader_emb, train=True)\n",
    "    val_loss, val_f1 = run_epoch_emb(valid_loader_emb, train=False)\n",
    "    emb_scheduler.step()\n",
    "    print({\"epoch\": epoch, \"train_loss\": round(train_loss, 4), \"train_f1\": round(train_f1, 4), \"val_loss\": round(val_loss, 4), \"val_f1\": round(val_f1, 4)})\n",
    "    if val_f1 > best_val_f1_emb:\n",
    "        best_val_f1_emb = val_f1\n",
    "        best_state_emb = deepcopy(emb_model.state_dict())\n",
    "        stale_emb = 0\n",
    "    else:\n",
    "        stale_emb += 1\n",
    "        if stale_emb >= patience_emb:\n",
    "            print(f\"Early stopping (emb) at epoch {epoch} (best val_f1={best_val_f1_emb:.4f})\")\n",
    "            break\n",
    "\n",
    "# Load best and evaluate on test (embeddings)\n",
    "assert best_state_emb is not None, \"No best embedding model state captured.\"\n",
    "emb_model.load_state_dict(best_state_emb)\n",
    "_, test_f1_emb = run_epoch_emb(test_loader_emb, train=False)\n",
    "print({\"best_val_f1_emb\": round(best_val_f1_emb, 4), \"test_f1_emb\": round(test_f1_emb, 4)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0664b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
