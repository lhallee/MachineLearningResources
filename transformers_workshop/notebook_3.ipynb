{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 25\n",
    "IMG_SIZE = 28\n",
    "HIDDEN_DIM = 128\n",
    "INTERMEDIATE_DIM = 256\n",
    "NUM_BLOCKS = 1\n",
    "NUM_CLASSES = 10\n",
    "PATIENCE = 3\n",
    "LOGGING_STEPS = 100\n",
    "ATTENTION_HEAD_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_dataset = datasets.MNIST('../data/MNIST/', download=True, train=True)\n",
    "test_dataset = datasets.MNIST('../data/MNIST/', download=True, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (28, 28), Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Image shape: {np.array(train_dataset[0][0]).shape}, Label: {np.array(train_dataset[0][1])}')\n",
    "plt.imshow(np.array(train_dataset[0][0]).squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13066047430038452, Std: 0.30810779333114624\n"
     ]
    }
   ],
   "source": [
    "# calcluate mean and std of the dataset\n",
    "mean = train_dataset.data.float().mean() / 255\n",
    "std = train_dataset.data.float().std() / 255\n",
    "print(f'Mean: {mean}, Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    ")\n",
    "train_dataset = datasets.MNIST('../data/MNIST/', download=True, train=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data/MNIST/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 28, 28]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for ex in test_loader:\n",
    "    print(ex[0].shape, ex[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __call__(self, batch: List[Tuple[torch.Tensor, int]]) -> Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            'x': torch.stack([f[0] for f in batch]),\n",
    "            'labels': torch.tensor([f[1] for f in batch])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sqrt_d = math.sqrt(hidden_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # (B, L, d)\n",
    "        Q = self.W_Q(x) # (B, L, d)\n",
    "        K = self.W_K(x) # (B, L, d)\n",
    "        V = self.W_V(x) # (B, L, d)\n",
    "        a = Q.matmul(K.permute(0, 2, 1)) / self.sqrt_d # (B, 50, 50)\n",
    "        a = a.softmax(dim=-1).matmul(V)\n",
    "        return a\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = ATTENTION_HEAD_SIZE\n",
    "        self.head_dim = hidden_dim // ATTENTION_HEAD_SIZE\n",
    "        self.W_Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W_V = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sqrt_d = math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch_size, length, hidden_dim)\n",
    "        batch_size = x.size(0)  \n",
    "        Q = self.W_Q(x).reshape(batch_size, -1, self.num_heads, self.head_dim) # (batch_size, length, num_heads, head_dim)\n",
    "        K = self.W_K(x).reshape(batch_size, -1, self.num_heads, self.head_dim) # (batch_size, length, num_heads, head_dim)\n",
    "        V = self.W_V(x).reshape(batch_size, -1, self.num_heads, self.head_dim) # (batch_size, length, num_heads, head_dim)\n",
    "        Q = Q.transpose(1, 2) # (batch_size, num_heads, length, head_dim)\n",
    "        K = K.transpose(1, 2) # (batch_size, num_heads, length, head_dim)\n",
    "        V = V.transpose(1, 2) # (batch_size, num_heads, length, head_dim)\n",
    "        K = K.transpose(2, 3) # (batch_size, num_heads, head_dim, length)\n",
    "        a = Q.matmul(K) / self.sqrt_d # (batch_size, num_heads, length, length)\n",
    "        a = a.softmax(dim=-1) # (batch_size, num_heads, length, length)\n",
    "        a = a.matmul(V) # (batch_size, num_heads, length, head_dim)\n",
    "        a = a.transpose(1, 2) # (batch_size, length, num_heads, head_dim)\n",
    "        a = a.reshape(batch_size, -1, self.hidden_dim) # (batch_size, length, hidden_dim)\n",
    "        return a\n",
    "        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim: torch.Tensor, intermediate_dim: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.w2 = nn.Linear(intermediate_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # (B, L, d)\n",
    "        return self.w2(F.relu(self.w1(x))) # (B, L, d)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim: torch.Tensor, intermediate_dim: torch.Tensor, multi_head_attention: Optional[bool] = False):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(hidden_dim) if not multi_head_attention else MultiHeadAttention(hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, intermediate_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        attention_output = self.attention(x)\n",
    "        attention_output = attention_output + residual\n",
    "        residual = attention_output\n",
    "        mlp_output = self.mlp(attention_output)\n",
    "        mlp_output = mlp_output + residual\n",
    "        return mlp_output\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_dim: torch.Tensor, num_classes: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.out(F.relu(self.dense(x)))\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size: torch.Tensor, hidden_size: torch.Tensor, num_classes: torch.Tensor):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: Optional[torch.Tensor] = None) -> ImageClassifierOutput:\n",
    "        # (batch_size, 1, 28, 28), (batch_size,)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes: torch.Tensor):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(64 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: Optional[torch.Tensor] = None) -> ImageClassifierOutput:\n",
    "        # (batch_size, 1, 28, 28), (batch_size,)\n",
    "        batch_size = x.size(0)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: torch.Tensor,\n",
    "        hidden_dim: torch.Tensor,\n",
    "        intermediate_dim: torch.Tensor,\n",
    "        num_classes: torch.Tensor,\n",
    "        num_blocks: torch.Tensor,\n",
    "        multi_head_attention: Optional[bool] = False\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(hidden_dim, intermediate_dim, multi_head_attention) for _ in range(num_blocks)])\n",
    "        self.classifier = Classifier(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: Optional[torch.Tensor] = None) -> ImageClassifierOutput:\n",
    "        # (batch_size, 1, 28, 28), (batch_size,)\n",
    "        batch_size = x.size(0)\n",
    "        img_size = x.size(-1)\n",
    "        x = x.view(batch_size, img_size, img_size) # (batch_size, 784)\n",
    "        x = self.embedding(x) # (batch_size, hidden_dim)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        h = x.mean(dim=1)\n",
    "        logits = self.classifier(h)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "SimpleNN                                 --\n",
      "├─Linear: 1-1                            100,480\n",
      "├─Linear: 1-2                            1,290\n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3123245239257812, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.1099, 'eval_runtime': 1.6377, 'eval_samples_per_second': 6106.159, 'eval_steps_per_second': 12.212}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2950 04:03 < 00:10, 11.60 it/s, Epoch 24/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.244571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.172660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.139337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.119170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.107854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.099480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.095508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.088064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.084197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.083278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.081129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.077676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.076167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.076196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.074518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.072873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.074715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.073030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.071995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.071663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.071691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07199540734291077, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.9787, 'eval_runtime': 1.3549, 'eval_samples_per_second': 7380.532, 'eval_steps_per_second': 14.761, 'epoch': 24.0}\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CNN                                      --\n",
      "├─Conv2d: 1-1                            320\n",
      "├─AvgPool2d: 1-2                         --\n",
      "├─Conv2d: 1-3                            18,496\n",
      "├─AvgPool2d: 1-4                         --\n",
      "├─Linear: 1-5                            31,370\n",
      "=================================================================\n",
      "Total params: 50,186\n",
      "Trainable params: 50,186\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.296696186065674, 'eval_model_preparation_time': 0.0002, 'eval_accuracy': 0.1212, 'eval_runtime': 1.7414, 'eval_samples_per_second': 5742.386, 'eval_steps_per_second': 11.485}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1416' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1416/2950 02:29 < 02:41, 9.48 it/s, Epoch 12/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.180234</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.079836</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.057256</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.051325</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.042505</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.039012</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.035558</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.035078</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03281313553452492, 'eval_model_preparation_time': 0.0002, 'eval_accuracy': 0.9894, 'eval_runtime': 1.6189, 'eval_samples_per_second': 6176.921, 'eval_steps_per_second': 12.354, 'epoch': 12.0}\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "ViT                                      --\n",
      "├─Linear: 1-1                            3,712\n",
      "├─ModuleList: 1-2                        --\n",
      "│    └─TransformerBlock: 2-1             --\n",
      "│    │    └─Attention: 3-1               49,536\n",
      "│    │    └─MLP: 3-2                     65,920\n",
      "├─Classifier: 1-3                        --\n",
      "│    └─Linear: 2-2                       16,512\n",
      "│    └─Linear: 2-3                       1,290\n",
      "=================================================================\n",
      "Total params: 136,970\n",
      "Trainable params: 136,970\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.323580265045166, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.0931, 'eval_runtime': 1.3804, 'eval_samples_per_second': 7244.186, 'eval_steps_per_second': 14.488}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2950' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2950/2950 04:50, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.478600</td>\n",
       "      <td>0.936568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.723295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718500</td>\n",
       "      <td>0.609032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>0.558628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.530199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.511752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>0.512207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.474623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.454495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.442804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.861700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.430815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.404800</td>\n",
       "      <td>0.447827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.854600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.420387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.418195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.396071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.388589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.391435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.379116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.382014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.370710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.365037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>0.370128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.358619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.357018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.353968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35701829195022583, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.8893, 'eval_runtime': 1.431, 'eval_samples_per_second': 6988.319, 'eval_steps_per_second': 13.977, 'epoch': 25.0}\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "ViT                                      --\n",
      "├─Linear: 1-1                            3,712\n",
      "├─ModuleList: 1-2                        --\n",
      "│    └─TransformerBlock: 2-1             --\n",
      "│    │    └─MultiHeadAttention: 3-1      49,536\n",
      "│    │    └─MLP: 3-2                     65,920\n",
      "├─Classifier: 1-3                        --\n",
      "│    └─Linear: 2-2                       16,512\n",
      "│    └─Linear: 2-3                       1,290\n",
      "=================================================================\n",
      "Total params: 136,970\n",
      "Trainable params: 136,970\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.323413133621216, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.096, 'eval_runtime': 1.8301, 'eval_samples_per_second': 5464.237, 'eval_steps_per_second': 10.928}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2950' max='2950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2950/2950 07:49, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.477800</td>\n",
       "      <td>0.791122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.749300</td>\n",
       "      <td>0.579075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.468249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.411330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.369321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>0.366126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.344248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.327650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.318145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.306457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.307306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.309097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>0.284217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.287831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.269560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.268613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.268573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.267533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.258218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.260494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.261107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.256993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.255643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.251130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.250057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2500567138195038, 'eval_model_preparation_time': 0.0, 'eval_accuracy': 0.9209, 'eval_runtime': 1.9035, 'eval_samples_per_second': 5253.349, 'eval_steps_per_second': 10.507, 'epoch': 25.0}\n",
      "{'Convolutional Neural Network': 0.9894, 'Simple Neural Network': 0.9787, 'Vision Transformer with Multi-Head Attention': 0.9209, 'Vision Transformer': 0.8893}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Simple Neural Network': SimpleNN(input_size=IMG_SIZE*IMG_SIZE, hidden_size=HIDDEN_DIM, num_classes=NUM_CLASSES),\n",
    "    'Convolutional Neural Network': CNN(num_classes=NUM_CLASSES),\n",
    "    'Vision Transformer': ViT(input_dim=IMG_SIZE, hidden_dim=HIDDEN_DIM, intermediate_dim=INTERMEDIATE_DIM, num_classes=NUM_CLASSES, num_blocks=NUM_BLOCKS),\n",
    "    'Vision Transformer with Multi-Head Attention': ViT(input_dim=IMG_SIZE, hidden_dim=HIDDEN_DIM, intermediate_dim=INTERMEDIATE_DIM, num_classes=NUM_CLASSES, num_blocks=NUM_BLOCKS, multi_head_attention=True),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(summary(model))\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollator(),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    "    )\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(metrics)\n",
    "    trainer.accelerator.free_memory()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    final_accuracy = metrics['eval_accuracy']\n",
    "    results[model_name] = final_accuracy\n",
    "\n",
    "\n",
    "# sort results by accuracy\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (embedding): Linear(in_features=28, out_features=128, bias=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (w1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (w2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (out): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer_model = trainer.model\n",
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "HFViT                                    --\n",
       "├─Linear: 1-1                            3,712\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─TransformerBlock: 2-1             --\n",
       "│    │    └─MultiHeadAttention: 3-1      49,536\n",
       "│    │    └─MLP: 3-2                     65,920\n",
       "├─Classifier: 1-3                        --\n",
       "│    └─Linear: 2-2                       16,512\n",
       "│    └─Linear: 2-3                       1,290\n",
       "=================================================================\n",
       "Total params: 136,970\n",
       "Trainable params: 136,970\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "\n",
    "class HFViTConfig(PretrainedConfig):\n",
    "    model_type = \"vit\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        num_hidden_layers: int,\n",
    "        num_classes: int,\n",
    "        multi_head_attention: bool,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.multi_head_attention = multi_head_attention\n",
    "\n",
    "\n",
    "\n",
    "class HFViT(PreTrainedModel):\n",
    "    config_class = HFViTConfig\n",
    "    def __init__(self, config: HFViTConfig):\n",
    "        super().__init__(config)\n",
    "        self.embedding = nn.Linear(config.input_dim, config.hidden_size)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(config.hidden_size, config.intermediate_size, config.multi_head_attention) for _ in range(config.num_hidden_layers)])\n",
    "        self.classifier = Classifier(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, labels: Optional[torch.Tensor] = None) -> ImageClassifierOutput:\n",
    "        # (batch_size, 1, 28, 28), (batch_size,)\n",
    "        batch_size = x.size(0)\n",
    "        img_size = x.size(-1)\n",
    "        x = x.view(batch_size, img_size, img_size) # (batch_size, 784)\n",
    "        x = self.embedding(x) # (batch_size, hidden_dim)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        h = x.mean(dim=1)\n",
    "        logits = self.classifier(h)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "\n",
    "config = HFViTConfig(input_dim=IMG_SIZE, hidden_size=HIDDEN_DIM, intermediate_size=INTERMEDIATE_DIM, num_hidden_layers=NUM_BLOCKS, num_classes=NUM_CLASSES, multi_head_attention=True)\n",
    "hf_model = HFViT(config)\n",
    "summary(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "HFViT                                    --\n",
       "├─Linear: 1-1                            3,712\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─TransformerBlock: 2-1             --\n",
       "│    │    └─MultiHeadAttention: 3-1      49,536\n",
       "│    │    └─MLP: 3-2                     65,920\n",
       "├─Classifier: 1-3                        --\n",
       "│    └─Linear: 2-2                       16,512\n",
       "│    └─Linear: 2-3                       1,290\n",
       "=================================================================\n",
       "Total params: 136,970\n",
       "Trainable params: 136,970\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.load_state_dict(transformer_model.state_dict())\n",
    "summary(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HFViTConfig.__init__() missing 6 required positional arguments: 'input_dim', 'hidden_size', 'intermediate_size', 'num_hidden_layers', 'num_classes', and 'multi_head_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlhallee/mnist-vit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3617\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[0;32m   3616\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[1;32m-> 3617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:975\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    972\u001b[0m files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(work_dir)\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# Update model card if needed:\u001b[39;00m\n\u001b[0;32m    978\u001b[0m model_card\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(work_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3337\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   3334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_main_process:\n\u001b[0;32m   3335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n\u001b[0;32m   3336\u001b[0m         \u001b[38;5;66;03m# If the model config has set attributes that should be in the generation config, move them there.\u001b[39;00m\n\u001b[1;32m-> 3337\u001b[0m         misplaced_generation_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_non_default_generation_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3338\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_generate() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(misplaced_generation_parameters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3339\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3340\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoving the following attributes in the config to the generation config: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3341\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmisplaced_generation_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. You are seeing this warning because you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3342\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration parameters in the model config, as opposed to in the generation config.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3343\u001b[0m                 \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   3344\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\configuration_utils.py:1079\u001b[0m, in \u001b[0;36mPretrainedConfig._get_non_default_generation_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;66;03m# Composite models don't have a default config, use their decoder config as a fallback for default values\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;66;03m# If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1079\u001b[0m     default_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1081\u001b[0m     decoder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_text_config(decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: HFViTConfig.__init__() missing 6 required positional arguments: 'input_dim', 'hidden_size', 'intermediate_size', 'num_hidden_layers', 'num_classes', and 'multi_head_attention'"
     ]
    }
   ],
   "source": [
    "hf_model.push_to_hub('lhallee/mnist-vit', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.load_from_hub('lhallee/mnist-vit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and quick comparisons\n",
    "We trained four models on MNIST with identical hyperparameters. Below we visualize final accuracies and inspect predictions for the last trained model (as used by `trainer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Bar chart of final accuracies\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      4\u001b[0m accs \u001b[38;5;241m=\u001b[39m [results[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Bar chart of final accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = list(results.keys())\n",
    "accs = [results[k] for k in names]\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.barh(names, accs)\n",
    "plt.xlabel('Accuracy'); plt.title('MNIST final accuracy by model')\n",
    "for i, v in enumerate(accs):\n",
    "    plt.text(v, i, f'{v:.3f}', va='center', ha='left')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and sample predictions for the last trainer.model\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "pred_output = trainer.predict(test_dataset)\n",
    "preds = pred_output.predictions.argmax(axis=-1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ConfusionMatrixDisplay(cm).plot(ax=ax, colorbar=False)\n",
    "ax.set_title('Confusion Matrix (last trained model)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show a grid of 20 sample predictions\n",
    "indices = np.random.choice(len(test_dataset), size=20, replace=False)\n",
    "fig, axes = plt.subplots(4, 5, figsize=(8,6))\n",
    "for ax, idx in zip(axes.ravel(), indices):\n",
    "    x, y = test_dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        logits = trainer.model(x.unsqueeze(0))\n",
    "        if hasattr(logits, 'logits'):\n",
    "            logits = logits.logits\n",
    "        pred = logits.argmax(dim=-1).item()\n",
    "    ax.imshow(x.squeeze().numpy(), cmap='gray')\n",
    "    ax.set_title(f't:{y}/p:{pred}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Random sample predictions (last trained model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
